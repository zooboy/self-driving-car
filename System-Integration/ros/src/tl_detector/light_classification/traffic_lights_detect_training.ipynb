{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, MaxPooling2D, Flatten,Lambda,Conv2D,BatchNormalization\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "def dataset_add(dataset_dir,ylabel,X=X,Y=Y):\n",
    "    diclist = os.listdir(dataset_dir)\n",
    "    for filename in diclist:\n",
    "        img_name = dataset_dir + filename\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.resize(img,(80,60))\n",
    "        X.append(img)\n",
    "        Y.append(ylabel)\n",
    "        flipimage = np.fliplr(img)\n",
    "        X.append(flipimage)\n",
    "        Y.append(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the dataset and ad flip images to augment the dataset.\n",
    "# the dataset mainly comes from two source: ROSbag file from Carla and part of LaRA Traffic Lights Recognition Dataset\n",
    "dataset_add(dataset_dir='./dataset2/clean/green_2/',ylabel =2)\n",
    "dataset_add(dataset_dir='./dataset2/clean/red_0/',ylabel =0)\n",
    "dataset_add(dataset_dir='./dataset2/clean/yellow_1/',ylabel =1)\n",
    "dataset_add(dataset_dir='./dataset2/clean/unknown_4/',ylabel =3)\n",
    "dataset_add(dataset_dir='./newsite/red/',ylabel =0)\n",
    "dataset_add(dataset_dir='./newsite/yellow/',ylabel =1)\n",
    "dataset_add(dataset_dir='./newsite/green/',ylabel =2)\n",
    "dataset_add(dataset_dir='./newsite/nol/',ylabel =3)\n",
    "dataset_add(dataset_dir='./addset/red/',ylabel =0)\n",
    "dataset_add(dataset_dir='./addset/green/',ylabel =2)\n",
    "dataset_add(dataset_dir='./addset/abg/',ylabel =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= to_categorical(np.array(Y))\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.10,random_state=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6884, 60, 80, 3)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6884, 4)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mcheckpoint = ModelCheckpoint('./tl_7.h5', monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                              save_weights_only=False, mode='auto', period=1)\n",
    "reducelr =ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n",
    "                            verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(60,80,3)))\n",
    "model.add(Conv2D(24,(5,5),strides=(2,2),activation='relu'))\n",
    "\n",
    "model.add(Conv2D(36,(3,3),strides=(2,2),activation='relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(Conv2D(48,(3,3),strides=(2,2),activation='relu'))\n",
    "model.add(Conv2D(64,(2,2),strides=(1,1),activation='relu'))\n",
    "model.add(Conv2D(64,(2,2),strides=(1,1),activation='relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6195 samples, validate on 689 samples\n",
      "Epoch 1/50\n",
      "8s - loss: 1.2663 - acc: 0.4342 - val_loss: 0.9323 - val_acc: 0.5210\n",
      "Epoch 2/50\n",
      "4s - loss: 0.9548 - acc: 0.5245 - val_loss: 0.8523 - val_acc: 0.5776\n",
      "Epoch 3/50\n",
      "4s - loss: 0.8492 - acc: 0.5727 - val_loss: 0.7734 - val_acc: 0.6226\n",
      "Epoch 4/50\n",
      "4s - loss: 0.7542 - acc: 0.6286 - val_loss: 0.6366 - val_acc: 0.7083\n",
      "Epoch 5/50\n",
      "4s - loss: 0.6838 - acc: 0.6743 - val_loss: 0.9281 - val_acc: 0.6023\n",
      "Epoch 6/50\n",
      "4s - loss: 0.6340 - acc: 0.7073 - val_loss: 0.5433 - val_acc: 0.7736\n",
      "Epoch 7/50\n",
      "4s - loss: 0.5202 - acc: 0.7985 - val_loss: 0.2775 - val_acc: 0.8882\n",
      "Epoch 8/50\n",
      "4s - loss: 0.3412 - acc: 0.8759 - val_loss: 0.2792 - val_acc: 0.8940\n",
      "Epoch 9/50\n",
      "4s - loss: 0.2960 - acc: 0.8973 - val_loss: 0.2414 - val_acc: 0.9028\n",
      "Epoch 10/50\n",
      "4s - loss: 0.2821 - acc: 0.9077 - val_loss: 0.1479 - val_acc: 0.9303\n",
      "Epoch 11/50\n",
      "4s - loss: 0.2457 - acc: 0.9209 - val_loss: 0.3652 - val_acc: 0.8766\n",
      "Epoch 12/50\n",
      "4s - loss: 0.2405 - acc: 0.9285 - val_loss: 1.0744 - val_acc: 0.8549\n",
      "Epoch 13/50\n",
      "4s - loss: 0.2200 - acc: 0.9362 - val_loss: 0.1174 - val_acc: 0.9608\n",
      "Epoch 14/50\n",
      "4s - loss: 0.1847 - acc: 0.9474 - val_loss: 0.0954 - val_acc: 0.9710\n",
      "Epoch 15/50\n",
      "4s - loss: 0.1793 - acc: 0.9529 - val_loss: 0.0775 - val_acc: 0.9608\n",
      "Epoch 16/50\n",
      "4s - loss: 0.1705 - acc: 0.9587 - val_loss: 0.0985 - val_acc: 0.9681\n",
      "Epoch 17/50\n",
      "4s - loss: 0.1533 - acc: 0.9608 - val_loss: 0.1031 - val_acc: 0.9579\n",
      "Epoch 18/50\n",
      "4s - loss: 0.1516 - acc: 0.9622 - val_loss: 0.3576 - val_acc: 0.9144\n",
      "Epoch 19/50\n",
      "4s - loss: 0.1394 - acc: 0.9684 - val_loss: 0.1441 - val_acc: 0.9681\n",
      "Epoch 20/50\n",
      "4s - loss: 0.1373 - acc: 0.9711 - val_loss: 0.0597 - val_acc: 0.9811\n",
      "Epoch 21/50\n",
      "4s - loss: 0.1228 - acc: 0.9706 - val_loss: 0.0578 - val_acc: 0.9884\n",
      "Epoch 22/50\n",
      "4s - loss: 0.1347 - acc: 0.9705 - val_loss: 0.0788 - val_acc: 0.9724\n",
      "Epoch 23/50\n",
      "4s - loss: 0.1245 - acc: 0.9727 - val_loss: 0.0532 - val_acc: 0.9797\n",
      "Epoch 24/50\n",
      "4s - loss: 0.1229 - acc: 0.9772 - val_loss: 0.1382 - val_acc: 0.9768\n",
      "Epoch 25/50\n",
      "4s - loss: 0.1138 - acc: 0.9756 - val_loss: 0.0751 - val_acc: 0.9869\n",
      "Epoch 26/50\n",
      "4s - loss: 0.1060 - acc: 0.9753 - val_loss: 0.0448 - val_acc: 0.9898\n",
      "Epoch 27/50\n",
      "4s - loss: 0.1112 - acc: 0.9742 - val_loss: 0.0311 - val_acc: 0.9884\n",
      "Epoch 28/50\n",
      "4s - loss: 0.0875 - acc: 0.9789 - val_loss: 0.0834 - val_acc: 0.9840\n",
      "Epoch 29/50\n",
      "4s - loss: 0.1016 - acc: 0.9790 - val_loss: 0.0297 - val_acc: 0.9927\n",
      "Epoch 30/50\n",
      "4s - loss: 0.0944 - acc: 0.9818 - val_loss: 0.0696 - val_acc: 0.9840\n",
      "Epoch 31/50\n",
      "4s - loss: 0.0964 - acc: 0.9803 - val_loss: 0.0222 - val_acc: 0.9956\n",
      "Epoch 32/50\n",
      "4s - loss: 0.1108 - acc: 0.9769 - val_loss: 0.0306 - val_acc: 0.9898\n",
      "Epoch 33/50\n",
      "4s - loss: 0.1045 - acc: 0.9805 - val_loss: 0.1077 - val_acc: 0.9782\n",
      "Epoch 34/50\n",
      "4s - loss: 0.0977 - acc: 0.9805 - val_loss: 0.0471 - val_acc: 0.9855\n",
      "Epoch 35/50\n",
      "4s - loss: 0.0741 - acc: 0.9858 - val_loss: 0.0554 - val_acc: 0.9826\n",
      "Epoch 36/50\n",
      "4s - loss: 0.0954 - acc: 0.9837 - val_loss: 0.0464 - val_acc: 0.9898\n",
      "Epoch 37/50\n",
      "5s - loss: 0.0907 - acc: 0.9834 - val_loss: 0.0475 - val_acc: 0.9942\n",
      "Epoch 38/50\n",
      "4s - loss: 0.0310 - acc: 0.9931 - val_loss: 0.0222 - val_acc: 0.9956\n",
      "Epoch 39/50\n",
      "4s - loss: 0.0300 - acc: 0.9950 - val_loss: 0.0307 - val_acc: 0.9956\n",
      "Epoch 40/50\n",
      "4s - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0365 - val_acc: 0.9942\n",
      "Epoch 41/50\n",
      "4s - loss: 0.0186 - acc: 0.9956 - val_loss: 0.0358 - val_acc: 0.9956\n",
      "Epoch 42/50\n",
      "4s - loss: 0.0215 - acc: 0.9958 - val_loss: 0.0304 - val_acc: 0.9956\n",
      "Epoch 43/50\n",
      "4s - loss: 0.0291 - acc: 0.9961 - val_loss: 0.0300 - val_acc: 0.9956\n",
      "Epoch 44/50\n",
      "4s - loss: 0.0167 - acc: 0.9963 - val_loss: 0.0307 - val_acc: 0.9956\n",
      "Epoch 45/50\n",
      "4s - loss: 0.0225 - acc: 0.9968 - val_loss: 0.0320 - val_acc: 0.9971\n",
      "Epoch 46/50\n",
      "4s - loss: 0.0162 - acc: 0.9976 - val_loss: 0.0333 - val_acc: 0.9971\n",
      "Epoch 47/50\n",
      "4s - loss: 0.0214 - acc: 0.9963 - val_loss: 0.0346 - val_acc: 0.9971\n",
      "Epoch 48/50\n",
      "4s - loss: 0.0177 - acc: 0.9971 - val_loss: 0.0345 - val_acc: 0.9971\n",
      "Epoch 49/50\n",
      "4s - loss: 0.0161 - acc: 0.9969 - val_loss: 0.0346 - val_acc: 0.9971\n",
      "Epoch 50/50\n",
      "4s - loss: 0.0185 - acc: 0.9964 - val_loss: 0.0347 - val_acc: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ba8d02a90>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,batch_size=32,epochs=50,verbose=2,\n",
    "          validation_data=(X_test,y_test),shuffle=True,callbacks =[reducelr,mcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tl_model = load_model('./tl_7.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdir = './testset/'\n",
    "diclist = os.listdir(testdir)\n",
    "test_x =[]\n",
    "test_y= []\n",
    "imglist=[]\n",
    "for filename in diclist:\n",
    "    img_name = testdir+ filename\n",
    "    img = cv2.imread(img_name)\n",
    "    img = cv2.resize(img,(80,60))\n",
    "    #img = img/255.0 -0.5\n",
    "    imglist.append(img_name)\n",
    "    test_x.append(img)\n",
    "    test_y.append(int(filename[-5]))\n",
    "test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tl_model.predict_classes(test_x,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 =  tl_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 2 2 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 2 2 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "prb = tl_model.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.56142812e-03,   2.26973090e-04,   3.54342829e-15,\n",
       "         3.03590481e-27,   1.59588159e-11,   1.00000000e+00,\n",
       "         1.00000000e+00,   5.22797450e-08,   1.82985495e-05,\n",
       "         2.08936114e-15,   3.25357319e-12,   7.07392882e-17,\n",
       "         4.42803513e-17,   1.68990065e-11], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prb[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./testset/left0197_1.jpg',\n",
       " './testset/left0203_1.jpg',\n",
       " './testset/left0230_0.jpg',\n",
       " './testset/left0264_0.jpg',\n",
       " './testset/left0324_0.jpg',\n",
       " './testset/left0362_2.jpg',\n",
       " './testset/left0452_2.jpg',\n",
       " './testset/left0528_1.jpg',\n",
       " './testset/left0529_1.jpg',\n",
       " './testset/left0539_0.jpg',\n",
       " './testset/left0547_0.jpg',\n",
       " './testset/left0567_0.jpg',\n",
       " './testset/left0577_0.jpg',\n",
       " './testset/left0597_0.jpg']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
